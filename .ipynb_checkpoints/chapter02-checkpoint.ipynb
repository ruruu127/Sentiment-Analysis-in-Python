{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Features from Reviews\n",
    "Imagine you are in the shoes of a company offering a variety of products. You want to know which of your products are bestsellers and most of all - why. We embark on step 1 of understanding the reviews of products, using a dataset with Amazon product reviews. To that end, we transform the text into a numeric form and consider a few complexities in the process.\n",
    "\n",
    "## Bag-of-words\n",
    "<video controls src=\"video/video2_1.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which statement about BOW is true?\n",
    "You were introduced to a bag-of-words(BOW) and some of its characteristics in the video. Which of the following statements about BOW **is** true?\n",
    "\n",
    "**Possible Answers**\n",
    "+ Bag-of-words preserves the word order and grammar rules. (Does it really preserve grammar rules and word order?)\n",
    "+ Bag-of-words describes the order and freqeuncy of words or tokens within a corpus of documents. (It describes the frequency but do you think it says anything about the word order?)\n",
    "+ **Bag-of-words is a simple but effective method to build a vocabulary of all the words occuring in a document.**\n",
    "+ Bag-of-words can only be applied to a large document, not to shorter documents or single sentences. (No, BOW is not limited to the size of a document. It works both on short and long documents.)\n",
    "\n",
    "That's correct! You'll next see how to apply this idea to sentiment analysis further.\n",
    "\n",
    "## Your first BOW\n",
    "### Exercise\n",
    "A bag-of-words is an approach to transform text to numeric form.\n",
    "\n",
    "In this exercise, you will apply a BOW to the `annak` list before moving on to a larger dataset in the next exercise.\n",
    "\n",
    "Your task will be to work with this list and apply a BOW using the `CountVectorizer()`. This transformation is your first step in being able to understand the sentiment of a text. Pay attention to words which might carry a strong sentiment.\n",
    "\n",
    "Remember that the output of a `CountVectorizer()` is a sparse matrix, which stores only entries which are non-zero. To look at the actual content of this matrix, we convert it to a dense array using the `.toarray()` method.\n",
    "\n",
    "Note that in this case you don't need to specify the `max_features` argument because the text is short.\n",
    "\n",
    "### Instructions\n",
    "+ Import the count vectorizer function from `sklearn.feature_extraction.text`.\n",
    "+ Build and fit the vectorizer on the small dataset.\n",
    "+ Create the BOW representation with name `anna_bow` by calling the `transform()` method.\n",
    "+ Print the BOW result as a dense array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You have transformed the first sentence of *Anna Karenina* to an array counting the frequencies of each word. However, the output is not very readable, is it? We are still missing the names of the features. And does the approach change when we apply it to a larger dataset? We explore these problems in the next exercise.\n",
    "\n",
    "## BOW using product reviews\n",
    "### Exercise\n",
    "You practiced a BOW on a small dataset. Now you will apply it to a sample of Amazon product reviews. The data has been imported for you and is called `reviews`. It contains two columns. The first one is called `score` and it is `0` when the review is negative, and `1` when it is positive. The second column is called `review` and it contains the text of the review that a customer wrote. Feel free to explore the data in the IPython Shell.\n",
    "\n",
    "Your task is to build a BOW vocabulary, using the `review` column.\n",
    "\n",
    "Remember that we can call the `.get_feature_names()` method on the vectorizer to obtain a list of all the vocabulary elements.\n",
    "\n",
    "### Instructions\n",
    "+ Create a CountVectorizer object, specifying the maximum number of features.\n",
    "+ Fit the vectorizer.\n",
    "+ Transform the fitted vectorizer.\n",
    "+ Create a DataFrame where you transform the sparse matrix to a dense array and make sure to correctly specify the names of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_csv(\"amazon_reviews_sample.csv\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have successfully built your first BOW generated vocabulary and transformed it to numeric features of the dataset!\n",
    "\n",
    "## Getting granular with n-grams\n",
    "<video controls src=\"video/video2_2.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify token sequence length with BOW\n",
    "### Exercise\n",
    "We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.\n",
    "\n",
    "In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the `review` column and specify the sequence length of tokens.\n",
    "\n",
    "### Instructions\n",
    "+ Build the vectorizer, specifying the token sequence length to be uni- and bigrams.\n",
    "+ Fit the vectorizer.\n",
    "+ Transform the fitted vectorizer.\n",
    "+ In the DataFrame, make sure to correctly specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You have built a numeric representation of the review column using uni- and bigrams!\n",
    "\n",
    "## Size of vocabulary of movies reviews\n",
    "### Exercise\n",
    "In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the `movies` reviews dataset. The first column is the `review`, which is of type `object` and the second column is the `label`, which is `0` for a negative review and 1 for a positive one.\n",
    "\n",
    "The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features.\n",
    "\n",
    "### Instructions\n",
    "1. Using the `movies` dataset, limit the size of the vocabulary to `100`.\n",
    "2. Using the `movies` dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents.\n",
    "3. Using the `movies` dataset, limit the size of the vocabulary to ignore terms which occur in no less than 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"IMDB_sample.csv\")\n",
    "\n",
    "#---------------\n",
    "# Instruction 1\n",
    "#---------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(\"INSTRUCTION 1:\", X_df.head())\n",
    "\n",
    "#---------------\n",
    "# Instruction 2\n",
    "#---------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(max_df=200)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(\"INSTRUCTION 1:\", X_df.head())\n",
    "\n",
    "#---------------\n",
    "# Instruction 3\n",
    "#---------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(\"INSTRUCTION 1:\", X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Any of the three methods you applied here can be used to limit the size of the vocabulary. Which of the three methods you used resulted in the lowest number of constructed features?\n",
    "\n",
    "## BOW with n-grams and vocabulary size\n",
    "### Exercise\n",
    "In this exercise, you will practice building a bag-of-words once more, using the `reviews` dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence.\n",
    "\n",
    "### Instructions\n",
    "+ Import the vectorizer from `sklearn`.\n",
    "+ Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.\n",
    "+ Fit the vectorizer to the review column.\n",
    "+ Create a DataFrame from the BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful job! You have successfully created a bag-of-words representation of the product reviews dataset, including more sophisticated sequence of tokens, while limiting the size of the vocabulary.\n",
    "\n",
    "## Build new features from text\n",
    "<video controls src=\"video/video2_3.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize a string from GoT\n",
    "### Exercise\n",
    "A first standard step when working with text is to tokenize it, in other words, split a bigger string into individual strings, which are usually single words (tokens).\n",
    "\n",
    "A string `GoT` has been created for you and it contains a quote from George R.R. Martin's *Game of Thrones*. Your task is to split it into individual tokens.\n",
    "\n",
    "### Instructions\n",
    "+ Import the word tokenizing function from `nltk`.\n",
    "+ Transform the `GoT` string to word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variables import GoT\n",
    "\n",
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great effort! You have successfully taken a string and split it up into word tokens.\n",
    "\n",
    "## Word tokens from the Avengers\n",
    "### Exercise\n",
    "Now that you have tokenized your first string, it is time to iterate over items of a list and tokenize them as well. An easy way to do that with one line of code is with a list comprehension.\n",
    "\n",
    "A list `avengers` has been created for you. It contains a few quotes from the *Avengers* movies. You can explore it in the IPython Shell.\n",
    "\n",
    "### Instructions\n",
    "+ Import the required function and package.\n",
    "+ Apply the word tokenizing function on each item of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variables import avengers\n",
    "\n",
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! You have built up on what you developed in the previous exercise and created a list comprehension where each of the items in the list is a quote from an Avengers movie.\n",
    "\n",
    "## A feature for the length of a review\n",
    "### Exercise\n",
    "You have now worked with a string and a list with string items, it is time to use a larger sample of data.\n",
    "\n",
    "You task in this exercise is to create a new feature for the length of a review, using the familiar `reviews` dataset.\n",
    "\n",
    "### Instructions\n",
    "1. \n",
    "\t+ Import the word tokenizing function from the required package.\n",
    "\t+ Apply the function to the `review` column of the `reviews` dataset.\n",
    "2. \n",
    "\t+ Iterate over the created `word_tokens` list.\n",
    "\t+ As you iterate, find the length of each item in the list and append it to the empty `len_tokens` list.\n",
    "\t+ Create a new feature `n_words` in the `reviews` for the length of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# Instruction 1\n",
    "#---------------\n",
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])\n",
    "\n",
    "#---------------\n",
    "# Instruction 2\n",
    "#---------------\n",
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You have used a list comprehension and a for loop to iterate over the word tokens created from the review column. You can employ the same approach to create other features, such as one counting the number of sentences in each review. This knowledge will also help you understand the next chapter.\n",
    "\n",
    "## Can you guess the language?\n",
    "<video controls src=\"video/video2_4.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idenfity the language of a string\n",
    "### Exercise\n",
    "Sometimes you might need to analyze the sentiment of non-English text. Your first task in such a case will be to identify the foreign language.\n",
    "\n",
    "In this exercise you will identify the language of a single string. A string called `foreign` has been created for you. Feel free to explore it in the IPython Shell.\n",
    "\n",
    "### Instructions\n",
    "+ Import the required function from the language detection package.\n",
    "+ Detect the language of the `foreign` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variables import foreign\n",
    "\n",
    "# Import the language detection function and package\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You have successfully identified the language of the string to be French!\n",
    "\n",
    "## Detect language of a list of strings\n",
    "### Exercise\n",
    "Now you will detect the language of each item in a list. A list called `sentences` has been created for you and it contains 3 sentences, each in a different language. They have been randomly extracted from the product reviews dataset.\n",
    "\n",
    "### Instructions\n",
    "+ Iterate over the sentences in the list.\n",
    "+ Detect the language of each sentence and append the detected language to the empty list `languages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variables import sentences\n",
    "from langdetect import detect_langs\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in range(len(sentences)):\n",
    "    languages.append(detect_langs(sentences[sentence]))\n",
    "    \n",
    "print('The detected languages are: ', languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! What languages did you detect?\n",
    "\n",
    "## Language detection of product reviews\n",
    "### Exercise\n",
    "You will practice language detection on a small dataset called `non_english_reviews`. It is a sample of non-English reviews from the Amazon product reviews.\n",
    "\n",
    "You will iterate over the rows of the dataset, detecting the language of each row and appending it to an empty list. The list needs to be cleaned so that it only contains the language of the review such as `'en'` for English instead of the regular output `en:0.9987654`. Remember that the language detection function might detect more than one language and the first item in the returned list is the most likely candidate. Finally, you will assign the list to a new column.\n",
    "\n",
    "The logic is the same as used in the slides and the exercise before but instead of applying the function to a list, you work with a dataset.\n",
    "\n",
    "### Instructions\n",
    "+ Iterate over the rows of the `non_english_reviews` dataset.\n",
    "+ Inside the loop, detect the language of the second column of the dataset.\n",
    "+ Clean the string by splitting on a `:` inside the list comprehension expression.\n",
    "+ Finally, assign the cleaned list to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(non_english_reviews)):\n",
    "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "print(non_english_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! You have succesfully built a new column in the dataset, which tells you in which language the respective review is written. This can be a very useful feature!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
