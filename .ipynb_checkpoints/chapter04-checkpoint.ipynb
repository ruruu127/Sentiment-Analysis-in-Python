{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Predict the Sentiment\n",
    "We employ machine learning to predict the sentiment of a review based on the words used in the review. We use logistic regression and evaluate its performance in a few different ways. These are some solid first models!\n",
    "\n",
    "## Let's predict the sentiment!\n",
    "<video controls src=\"video/video4_1.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression of movie reviews\n",
    "### Exercise\n",
    "In the video we learned that logistic regression is a common way to model a classification task, such as classifying the sentiment as positive or negative.\n",
    "\n",
    "In this exercise, you will work with the `movies` reviews dataset. The `label` column stores the sentiment, which is `1` when the review is positive, and `0` when negative. The text review has been transformed, using BOW, to numeric columns.\n",
    "\n",
    "Your task is to build a logistic regression model using the `movies` dataset and calculate its accuracy.\n",
    "\n",
    "### Instructions\n",
    "+ Import the logistic regression function.\n",
    "+ Create and fit a logistic regression on the labels `y` and the features `X`.\n",
    "+ Calculate the accuracy of the logistic regression model, using the default `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "df_movies = pd.read_csv(\"IMDB_sample.csv\")\n",
    "vect = CountVectorizer(max_features=200, token_pattern=r'[A-Za-z]+', stop_words=ENGLISH_STOP_WORDS)\n",
    "vect.fit(df_movies.review)\n",
    "vect_trans = vect.transform(df_movies.review)\n",
    "movies = pd.DataFrame(vect_trans.toarray(), columns=vect.get_feature_names())\n",
    "movies[\"label\"] = df.label\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the vector of targets and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You have built your first logistic regression model and checked its accuracy! Let's practice some more!\n",
    "\n",
    "## Logistic regression using Twitter data\n",
    "### Exercise\n",
    "In this exercise, you will build a logistic regression model using the `tweets` dataset. The target is given by the `airline_sentiment`, which is `0` for negative tweets, `1` for neutral, and `2` for positive ones. So, in this case, you are given a multi-class classification task. Everything we learned about binary problems applies to multi-class classification problems as well.\n",
    "\n",
    "You will evaluate the accuracy of the model using the two different approaches from the slides.\n",
    "\n",
    "The logistic regression function and accuracy score have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "+ Build and fit a logistic regression model using the defined `X` and `y` as arguments.\n",
    "+ Calculate the accuracy of the logistic regression model.\n",
    "+ Predict the labels.\n",
    "+ Calculate the *accuracy score* using the predicted and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(\"Tweets.csv\")\n",
    "df_tweets = df_tweets.loc[:, [\"airline_sentiment\", \"airline_sentiment_confidence\", \"retweet_count\", \"airline\", \"text\", \"negativereason\"]]\n",
    "df_tweets = pd.get_dummies(df_tweets, columns = [\"airline\", \"negativereason\"])\n",
    "#df_tweets.columns\n",
    "df_tweets.rename(columns={'negativereason_Bad Flight': \"Bad Flight\", \"negativereason_Can't Tell\": \"Can't Tell\",\n",
    "       'negativereason_Cancelled Flight': \"Cancelled Flight\",\n",
    "       'negativereason_Customer Service Issue': \"Customer Service Issue\",\n",
    "       'negativereason_Damaged Luggage': \"Damaged Luggage\",\n",
    "       'negativereason_Flight Attendant Complaints': \"Flight Attendant Complains\",\n",
    "       'negativereason_Flight Booking Problems': \"Flight Booking Problems\", 'negativereason_Late Flight': \"Late Flight\",\n",
    "       'negativereason_Lost Luggage': \"Lost Luggage\", 'negativereason_longlines': \"longlines\"}, inplace=True)\n",
    "\n",
    "vect = CountVectorizer(max_features=100, token_pattern=r'[A-Za-z]+', stop_words=ENGLISH_STOP_WORDS)\n",
    "vect.fit(df_tweets.text)\n",
    "vect_trans = vect.transform(df_tweets.text)\n",
    "tweets = pd.DataFrame(vect_trans.toarray(), columns=vect.get_feature_names())\n",
    "tweets = pd.concat([df_tweets, tweets], axis=1)\n",
    "tweets[\"airline_sentiment\"] = tweets[\"airline_sentiment\"].replace([\"negative\", \"neutral\", \"positive\"], [0, 1, 2])\n",
    "tweets.drop(\"text\", axis=1, inplace=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the vector of targets and matrix of features\n",
    "y = tweets.airline_sentiment\n",
    "X = tweets.drop('airline_sentiment', axis=1)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))\n",
    "\n",
    "# Create an array of prediction\n",
    "y_predict = log_reg.predict(X)\n",
    "\n",
    "# Print the accuracy using accuracy score\n",
    "print('Accuracy of logistic regression: ', accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You have built another logistic regression model and calculated its accuracy in two different ways. Have you noticed how the calculated accuracy scores are the same? This will not always be the case for other methods because the `.score()` function can use other default model performance metrics. So, use `.accuracy_score()` to be certain that you are calculating the accuracy when you are training a different supervised learning model.\n",
    "\n",
    "## Did we really predict the sentiment well?\n",
    "<video controls src=\"video/video4_2.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and assess a model: movies reviews\n",
    "### Exercise\n",
    "In this problem, you will build a logistic regression model using the `movies` dataset. The score is stored in the `label` column and is `1` when the review is positive, and `0` when negative. The text review has been transformed, using BOW, to numeric columns.\n",
    "\n",
    "You have already built a classifier but evaluated it using the same data employed in the training step. Make sure you now assess the model using an unseen test dataset. How does the performance of the model change when evaluated on the test set?\n",
    "\n",
    "### Instructions\n",
    "+ Import the function required for a train/test split.\n",
    "+ Perform the train/test split, specifying that 20% of the data should be used as a test set.\n",
    "+ Train a logistic regression model.\n",
    "+ Print out the accuracy of the model on the training and on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the vector of labels and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a logistic regression model and print out the accuracy\n",
    "log_reg = LogisticRegression().fit(X_train,y_train)\n",
    "print('Accuracy on train set: ', log_reg.score(X_train, y_train))\n",
    "print('Accuracy on test set: ', log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Did you notice how the logistic regression's accuracy decreases when we evaluate it on the test set instead of on the training set? It's normal to observe a small drop but if the decrease is large, this could be a signal that your model will not generalize well and will do poorly when evaluating new movie reviews.\n",
    "\n",
    "## Performance metrics of Twitter data\n",
    "### Exercise\n",
    "You will train a logistic regression model that predicts the sentiment of tweets and evaluate its performance on the test set using different metrics.\n",
    "\n",
    "A matrix `X` has been created for you. It contains features created with a BOW on the `text` column.\n",
    "\n",
    "The labels are stored in a vector called `y`. Vector `y` is `0` for negative tweets, `1` for neutral, and `2` for positive ones.\n",
    "Note that although we have 3 classes, it is still a classification problem. The accuracy still measures the proportion of correctly predicted instances. The confusion matrix will now be of size 3x3, each row will give the number of predicted cases for classes 2, 1, and 0, and each column - the true number of cases in class 2, 1, and 0.\n",
    "\n",
    "All required packages have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "+ Perform the train/test split, and stratify by `y`.\n",
    "+ Train a a logistic regression classifier.\n",
    "+ Predict the performance on the test set.\n",
    "+ Print the accuracy score and confusion matrix obtained on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy score test set: ', accuracy_score(y_test, y_predicted))\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_predicted)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Although the sentiment category here has 3 classes instead of 2, the way we trained and evaluated the model is the same as with 2 classes. The accuracy on the test data was good and the confusion matrix can also show us which category we are bad at predicting.\n",
    "\n",
    "## Build and assess a model: product review data\n",
    "### Exercise\n",
    "In this exercise, you will build a logistic regression using the `reviews` dataset, containing customers' reviews of Amazon products. The array `y` contains the sentiment : `1` if positive and `0` otherwise. The array `X` contains all numeric features created using a BOW approach. Feel free to explore them in the IPython Shell.\n",
    "\n",
    "Your task is to build a logistic regression model and calculate the accuracy and confusion matrix using the test data set.\n",
    "\n",
    "The logistic regression and train/test splitting functions have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "+ Import the accuracy score and confusion matrix functions.\n",
    "+ Split the data into training and testing, using 30% of it as a test set and set the random seed to `42`.\n",
    "+ Train a logistic regression model.\n",
    "+ Print out the accuracy score and confusion matrix using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels \n",
    "y_predict = log_reg.predict(X_test)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy score of test data: ', accuracy_score(y_test, y_predict))\n",
    "print('Confusion matrix of test data: \\n', confusion_matrix(y_test, y_predict)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You have successfully built another logistic regression model and evaluated its performance on the test set. Is there any way we can improve the performance of the model? We will discuss that in our next video!\n",
    "\n",
    "## Logistic regression: revisited\n",
    "<video controls src=\"video/video4_3.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict probabilities of movie reviews\n",
    "### Exercise\n",
    "In this problem, you will build a logistic regression using the `movies` dataset. The labels are stored in the array `y` and the features in `X`.\n",
    "\n",
    "Train the model on the training data. Instead of predicting classes, predict the probabilities that each instance in the test set belongs to each of the two classes.\n",
    "\n",
    "The logistic regression and train/test splitting functions have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "+ Split the data into training and testing set.\n",
    "+ Train a logistic regression model.\n",
    "+ Predict the probabilities for class 0 and for class 1 of the testing data. Class 0 is located as the first column in the predicted probabilities, and class 1 is the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the probability of the 0 class\n",
    "prob_0 = log_reg.predict_proba(X_test)[:, 0]\n",
    "# Predict the probability of the 1 class\n",
    "prob_1 = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"First 10 predicted probabilities of class 0: \", prob_0[:10])\n",
    "print(\"First 10 predicted probabilities of class 1: \", prob_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Did you notice how the probabilities of class 0 and class 1 add up to 1 for each instance? In problems where the proportion of one class is larger than the other, we might want to work with predicted probabilities instead of predicted classes.\n",
    "\n",
    "## Product reviews with regularization\n",
    "### Exercise\n",
    "In this exercise, you will work once more with the `reviews` dataset of Amazon product reviews. A vector of labels y contains the sentiment : `1` if positive and `0` otherwise. The matrix `X` contains all numeric features created using a BOW approach.\n",
    "\n",
    "You will need to train two logistic regression models with different levels of regularization and compare how they perform on the test data. Remember that regularization is a way to control the complexity of the model. The more regularized a model is, the less flexible it is but the better it can generalize. Models with higher level of regularization are often less accurate than non-regularized ones.\n",
    "\n",
    "### Instructions\n",
    "+ Split the data into a train and test sets.\n",
    "+ Train a logistic regression with regularization parameter of `1000`. Train a second logistic regression with regularization parameter equal to `0.001`.\n",
    "+ Print the accuracy scores of both models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train a logistic regression with regularization of 1000\n",
    "log_reg1 = LogisticRegression(C=1000).fit(X_train, y_train)\n",
    "# Train a logistic regression with regularization of 0.001\n",
    "log_reg2 = LogisticRegression(C=0.001).fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracies\n",
    "print('Accuracy of model 1: ', log_reg1.score(X_test, y_test))\n",
    "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Did you notice how the model with higher degree of penalization (low C) has lower accuracy than the one with very little penalization (high C)? We often sacrifice some accuracy when we regularize a model but the benefit is lower complexity and lower chance of overfitting.\n",
    "\n",
    "## Regularizing models with Twitter data\n",
    "### Exercise\n",
    "You will work with the Twitter data expressing customers' sentiment about airline companies. The `X` matrix of features and `y` vector of labels have been created for you. In addition, the training and testing split has been performed. You can work with the `X_train`, `X_test`, `y_train` and `y_test` arrays directly.\n",
    "\n",
    "You will train regularized and a more flexible models and evaluate them using different model performance metrics.\n",
    "\n",
    "All required packages have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "+ Train two logistic regressions: one with regularization parameter of 100 and a second of 0.1.\n",
    "+ Print the accuracy scores of both models.\n",
    "+ Print the confusion matrix of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression with regularizarion parameter of 100\n",
    "log_reg1 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "# Build a logistic regression with regularizarion parameter of 0.1\n",
    "log_reg2 = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for each model\n",
    "y_predict1 = log_reg1.predict(X_test)\n",
    "y_predict2 = log_reg2.predict(X_test)\n",
    "\n",
    "# Print performance metrics for each model\n",
    "print('Accuracy of model 1: ', log_reg1.score(X_test, y_test))\n",
    "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))\n",
    "print('Confusion matrix of model 1: \\n' , confusion_matrix(y_test, y_predict1)/len(y_test))\n",
    "print('Confusion matrix of model 2: \\n', confusion_matrix(y_test, y_predict2)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent effort! You have trained a more and less flexible logistic regressions to predict the sentiment of tweets and evaluated them using different performance metrics. In this case, we again sacrificed some accuracy when we imposed regularizarion.\n",
    "\n",
    "## Bringing it all together\n",
    "<video controls src=\"video/video4_4.mp4\" width=720>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Word cloud and feature creation\n",
    "### Exercise\n",
    "You will work with a sample of the `reviews` dataset throughout this exercise. It contains the `review` and `score` columns. Feel free to explore it in the IPython Shell.\n",
    "\n",
    "In the first step, you will build a word cloud using only positive reviews. The string `positive_reviews` has been created for you by concatenating the top 100 positive reviews.\n",
    "\n",
    "In the second step, you will create a new feature for the length of each review and add that new feature to the dataset.\n",
    "\n",
    "All the functions needed to plot a word cloud have been imported for you, as well as the `word_tokenize` function from the `nltk` module.\n",
    "\n",
    "### Instructions\n",
    "1. \n",
    "\t+ Call and create a word cloud image using the `positive_reviews`.\n",
    "\t+ Display the generated image.\n",
    "2. \n",
    "\t+ Tokenize each item in the `review` column, using the word tokenizing function we have been working with.\n",
    "\t+ Iterate over the created `word_tokens` list and find the length of each item in the list. Append that length to the empty `len_tokens` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "cloud_positives = WordCloud(background_color='white').generate(positive_reviews)\n",
    " \n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(cloud_positives, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each item in the review column\n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Create an empty list to store the length of the reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Which words stood out in the word cloud image? After you have successfully created a feature about the number of tokens in each review, it is time to transform the text of the review.\n",
    "\n",
    "## Step 2: Building a vectorizer\n",
    "### Exercise\n",
    "In this exercise, you are asked to build a TfIdf transformation of the `review` column in the `reviews` dataset. You are asked to specify the n-grams, stop words, the pattern of tokens and the size of the vocabulary arguments.\n",
    "\n",
    "This is the last step before we train a classifier to predict the sentiment of a review.\n",
    "\n",
    "### Instructions\n",
    "+ Import the Tfidf vectorizer and the default list of English stop words.\n",
    "+ Build the Tfidf vectorizer, specifying - in this order - the following arguments: use as stop words the default list of English stop words; as n-grams use uni- and bi-grams;the maximum number of features should be 200; capture only words using the specified pattern.\n",
    "+ Create a DataFrame using the Tfidf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer and default list of English stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Build the vectorizer\n",
    "vect = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=200, token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(reviews.review)\n",
    "# Create sparse matrix from the vectorizer\n",
    "X = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame\n",
    "reviews_transformed = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: \\n', reviews_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! You have transfomed the text column using the TfidfVectorizer and created 200 numeric columns from the review. You are now ready to build a binary classifier predicting the sentiment of a review.\n",
    "\n",
    "## Step 3: Building a classifier\n",
    "### Exercise\n",
    "This is the last step in the sentiment analysis prediction. We have explored and enriched our dataset with features related to the sentiment, and created numeric vectors from it.\n",
    "\n",
    "You will use the dataset that you built in the previous steps. Namely, it contains a feature for the length of reviews, and 200 features created with the Tfidf vectorizer.\n",
    "\n",
    "Your task is to train a logistic regression to predict the sentiment. The data has been imported for you and is called `reviews_transformed`. The target is called `score` and is binary : `1` when the product review is positive and `0` otherwise.\n",
    "\n",
    "Train a logistic regression model and evaluate its performance on the test data. How well does the model do?\n",
    "\n",
    "All the required packages have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "+ Perform the train/test split, allocating 20% of the data to testing and setting the random seed to `456`.\n",
    "+ Train a logistic regression model.\n",
    "+ Predict the class.\n",
    "+ Print out the accuracy score and the confusion matrix on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "y = reviews_transformed.score\n",
    "X = reviews_transformed.drop('score', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=456)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "# Predict the labels\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "\n",
    "# Print accuracy score and confusion matrix on test set\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test, y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Wrap up\n",
    "<video controls src=\"video/video4_5.mp4\" width=720>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
